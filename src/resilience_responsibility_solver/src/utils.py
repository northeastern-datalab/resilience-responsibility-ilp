
import pulp
import random
import multiprocessing
import networkx as nx
import numpy as np
import pandas as pd

from src.constants import queries, dissociated_queries, permuted_queries, query_ptime_linearizations

def remove_dominated_atoms(query):
    """
    For a given conjunctive query, removes all the tables that are dominated

    Args:
        query (list): A conjunctive query
    """
    dominated_tables = []
    for t1, tv1 in query:
        for _, tv2 in query:
            if set(tv2).issubset(set(tv1)) and set(tv1)!=set(tv2):
                dominated_tables += t1
    domination_free_query = list(filter(lambda x: x[0] not in dominated_tables, query))
    return domination_free_query

def remove_fully_dominated_atoms(query):
    """
    For a given conjunctive query, removes all the tables that are fully dominated

    Args:
        query (list): A conjunctive query
    """
    dominated_tables = []
    for t1, tv1 in query:
        dominated_variables = set()
        for _, tv2 in query:
            if set(tv2).issubset(set(tv1)) and set(tv1)!=set(tv2):
                dominated_variables = dominated_variables.union(set(tv2))
        if dominated_variables == set(tv1):
            dominated_tables += t1
    domination_free_query = list(filter(lambda x: x[0] not in dominated_tables, query))
    return domination_free_query

def addLPVariable(key, variable_dict, lp_type='ILP'):
    """
    Creates an LP / ILP Decision variable and adds it to a list if it does not already exist

    Args:
        key (str): Variable name
        variable_dict (dict): Dict of existing variables
        varType (str, optional): Denotes type of optimization problem. Defaults to 'ILP'.
    """

    variable_type = {'ILP': 'Binary', 'LP': 'Continuous'}
    if not key in variable_dict:
        variable_dict[key] = pulp.LpVariable(key, lowBound = 0, upBound = 1, cat = variable_type[lp_type])

def computeWitnesses(query, database_instance):
    """    
    Perform joins to obtain witnesses given a database instance (tuples in set of table)

    Args:
        query (list): A boolean conjunctive query described by the variables in each table
        database_instance (list): A list of tuples present in each table
    Returns:
        witnesses (DataFrame): The witnesses generated by the database instance under the given query
    """

    databaseInstanceDF = []
    for (table_name, table_columns) in query:
        tableDF = pd.DataFrame(database_instance[table_name], columns=table_columns)
        
        
        filter_conflicting_columns =  tableDF.groupby(tableDF.columns,axis=1).agg(lambda x: x.eq(x.iloc[:, 0], axis=0).all(1))
        tableDF = tableDF[filter_conflicting_columns].reset_index(drop = True).groupby(tableDF.columns,axis=1).first()

        databaseInstanceDF.append(tableDF)
    witnesses = pd.DataFrame([0], columns=list(['key']))
    for table in databaseInstanceDF:
        table['key'] = 0
        witnesses = pd.merge(witnesses, table)
    witnesses = witnesses.drop('key', axis=1)
    
    return witnesses

def performSemijoinReduction(query, database_instance):
    """
    Removes the tuples that do not participate in the query
    Args:
        query (list): A boolean conjunctive query described by the variables in each table
        database_instance (list): A list of tuples present in each table
    Returns:
        pruned_database_instance (DataFrame): Instance with only the tuples that participate in the query
    """
    witnesses = computeWitnesses(query, database_instance)
    pruned_database_instance = {table: pd.DataFrame() for table in database_instance.keys()}
    for tablename, table_vars in query:
        all_tuples_list = witnesses[table_vars]
        if len(pruned_database_instance[tablename]) == 0:
            pruned_database_instance[tablename] = all_tuples_list
        else:
            pruned_database_instance[tablename].columns = all_tuples_list.columns
            pruned_database_instance[tablename] = pd.concat([pruned_database_instance[tablename], all_tuples_list], axis=0, ignore_index=True)
    
    for table in database_instance.keys():
        pruned_database_instance[table] = pruned_database_instance[table].drop_duplicates()
        pruned_database_instance[table] = pruned_database_instance[table].values.tolist()

    
    return pruned_database_instance

def addTuples(query, step_size, domain_size, instance_data, bag_semantics = False, max_bag_size = 10):
    """
    Adds step_size new tuples to a given instance
    The new tuples are split over all tables in a random distribution.

    Args:
        query (list): A boolean conjunctive query described by the variables in each table
        step_size (int): Number of tuples to be added to the instance
        domain_size (int): Max domain size of resulting instance
        instance_data (list): Pre-existing instance including the database_instance and tuple_weights
        bag_semantics (bool, optional). Indicates if bag semantics or set semantics is used. Defaults to False.
        max_bag_size (int). Size of the largest bag possible
    """

    
    tables = set((table_name, len(table_variables)) for (table_name, table_variables) in query)

    numberOfTables = len(tables)
    if instance_data == None:
        database_instance = {t: set() for (t,_) in tables}
        tuple_weights = {}
    else:
        (database_instance, tuple_weights) = instance_data


    
    table_distribution = [0] * numberOfTables
    for _ in range(step_size):
        table_distribution[random.randint(0, numberOfTables - 1)] += 1

    index = 0
    for (table_name, arity) in tables:
        maxTuples = domain_size ** arity

        for _ in range(table_distribution[index]):
            if len(database_instance[table_name]) >= maxTuples:
                break
            else:
                t = tuple(np.random.randint(0, domain_size, size = arity))
                database_instance[table_name].add(t)
        index += 1

    if bag_semantics:
        
        
        for (table_name, arity) in tables:
            table = database_instance[table_name]
            for t in table:
                tuple_key = table_name + '_' + '_'.join(str(x) for x in t)
                if tuple_key not in tuple_weights:
                    tuple_weights[tuple_key] = np.random.randint(1, max_bag_size)
            
    return (database_instance, tuple_weights)

def pickRandomResponsibilityTuple(query, database_instance, resp_table):
    """
    For a given query and database instance, a random tuple from a given table

    Args:
        query (list): A boolean conjunctive query described by the variables in each table
        database_instance (dict): A collection of database table
        resp_table (str): table name from which to create database instance

    Returns:
        A key for a tuple randomly chosen from the table.
    """

    witnesses = computeWitnesses(query, database_instance)
    if len(witnesses) == 0:
        return None
    resp_table_in_query = list(filter(lambda x: x[0] == resp_table,query))
    
    resp_table_variables = random.choice(resp_table_in_query)[1]
    possible_resp_tuples = witnesses[resp_table_variables].drop_duplicates().values.tolist()
    resp_tuple = random.choice(possible_resp_tuples)
    resp_tuple = resp_table + "_" + "_".join(map(str, resp_tuple))

    return resp_tuple

def constant_tuple_linearization_of_query(query):
    """
    There are two ways to linearize a query
    1. Constant-tuples linearization: We keep the same tuples, and build the flow graph in different orderings of the query
    2. Constant-witness linearization: We generate the witnesses, and dissociate the query to be in line with this witnesses

    In constant tuple linearization, we do not change the input tuple or the query,
    rather we allow spurious witnesses to be created in the flow graph depending on the ordering.
    There are as many linearizations as permutations of the query.
    """
    
    query_name = ""
    for qn, q in queries.items():
        if q == query:
            query_name = qn

    return permuted_queries[query_name]

def constant_witness_linearization_of_query(query, database_instance, responsibility_tuples = [], tuple_weights = {}, ptime_linearization = False):
    """
    There are two ways to linearize a query
    1. Constant-tuples linearization: We keep the same tuples, and build the flow graph in different orderings of the query
    2. Constant-witness linearization: We generate the witnesses, and dissociate the query to be in line with this witnesses

    In constant witness linearization, the number of witnesses does not change but rather we change the variables in the table to make it a running intersection query
    """

    witnesses = computeWitnesses(query, database_instance)

    if type(responsibility_tuples) != list:
        responsibility_tuples = [responsibility_tuples]

    linearized_instances = []

    query_name = ""
    for qn, q in queries.items():
        if q == query:
            query_name = qn

    if ptime_linearization:
        linearized_queries= query_ptime_linearizations[query_name]
    else:
        linearized_queries= dissociated_queries[query_name]

    for mlq in linearized_queries:
        
        modified_database_instance = dict()
        for t, v in mlq:
            modified_database_instance[t] = witnesses[v].drop_duplicates()

        
        modified_responsibility_tuples = []
        for r in responsibility_tuples:
            modified_responsibility_tuples += get_mapped_new_tuples_after_dissociation(query, r, mlq, modified_database_instance)


        
        modified_tuple_weights = {}
        for t in tuple_weights:
            mapped_t = get_mapped_new_tuples_after_dissociation(query, t, mlq, modified_database_instance)
            for new_tuple in mapped_t:
                modified_tuple_weights[new_tuple] = tuple_weights[t]

        if len(responsibility_tuples) > 0:
            linearized_instances.append((mlq, modified_database_instance, modified_responsibility_tuples, modified_tuple_weights))
        else:
            linearized_instances.append((mlq, modified_database_instance, modified_tuple_weights))
      
    return linearized_instances

def get_mapped_new_tuples_after_dissociation(query, original_tuple, lq, modified_database_instance):
    """
    When a query is transformed with constant witness linearization, a tuple in the original db can be mapped into several tuples in the linearized db
    This function maps from the original tuple key to the new tuple keys

    Args:
        query (list): the original query
        original_tuple (str): a tuple in the original db
        lq (list): the linearized conjunctive query
        modified_database_instance (list): the db after linearization

    Returns:
        list: a list of keys of the new tuples
    """
    original_tuple = original_tuple.split('_') 
    original_table = original_tuple[0]
    
    original_variables = []
    for t,v in query:
        if t == original_table:
            original_variables = v
    
    tuple_candidate_rows = modified_database_instance[original_table].copy()
    z = list(zip(original_variables, original_tuple[1:]))
    for i in range(len(z)):
        tuple_candidate_rows = tuple_candidate_rows.loc[ tuple_candidate_rows[ z[i][0] ] == int(z[i][1]) ] 
        
    
    tuple_candidate_rows = tuple_candidate_rows.values.tolist()
    new_tuple_keys = []

    for new_tuple in tuple_candidate_rows:
        new_tuple_keys.append( original_table +'_' + '_'.join(str(x) for x in new_tuple))
    
    return new_tuple_keys

def nx_minimum_cut_with_timeout_inner(graph, results_dict):
    results_dict['cut_value'] = nx.minimum_cut(graph, 'source', 'target')


def nx_minimum_cut_with_timeout(graph, time_limit):
    """
    Executes the networkx minimum cut with a specified timeout 
    Returns None if time out is exceeded

    Args:
        graph (networkx Digraph): Graph to perform min cut
        time_limit (seconds): time limit in seconds
    """


    results_dict = dict()
    p = multiprocessing.Process(target=nx_minimum_cut_with_timeout_inner, args=(graph, results_dict))
    p.start()
    
    if time_limit is None:
        return nx.minimum_cut(graph, 'source', 'target')[0]
    p.join(time_limit)
    if p.is_alive():
        
        p.terminate()
        
        
        p.join()
    if 'cut_value' in results_dict:
        return results_dict['cut_value']
    else:
        return None